@article{zhang2021survey,
title = {A survey on federated learning},
journal = {Knowledge-Based Systems},
volume = {216},
pages = {106775},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.106775},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121000381},
author = {Chen Zhang and Yu Xie and Hang Bai and Bin Yu and Weihong Li and Yuan Gao},
keywords = {Federated learning, Privacy protection, Machine learning},
abstract = {Federated learning is a set-up in which multiple clients collaborate to solve machine learning problems, which is under the coordination of a central aggregator. This setting also allows the training data decentralized to ensure the data privacy of each device. Federated learning adheres to two major ideas: local computing and model transmission, which reduces some systematic privacy risks and costs brought by traditional centralized machine learning methods. The original data of the client is stored locally and cannot be exchanged or migrated. With the application of federated learning, each device uses local data for local training, then uploads the model to the server for aggregation, and finally the server sends the model update to the participants to achieve the learning goal. To provide a comprehensive survey and facilitate the potential research of this area, we systematically introduce the existing works of federated learning from five aspects: data partitioning, privacy mechanism, machine learning model, communication architecture and systems heterogeneity. Then, we sort out the current challenges and future research directions of federated learning. Finally, we summarize the characteristics of existing federated learning, and analyze the current practical application of federated learning.}
}

@misc{mcmahan2017communication,
      title={Communication-Efficient Learning of Deep Networks from Decentralized Data}, 
      author={H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
      year={2023},
      eprint={1602.05629},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.05629}, 
}

@inbook{yang2020,
      author={Yang, Qiang and Liu, Yang and Cheng, Yong and Kang, Yan and Chen, Tianjian and Yu, Han},
      title={Federated Transfer Learning},
      bookTitle={Federated Learning},
      year={2020},
      publisher={Springer International Publishing},
      address={Cham},
      pages={83--93},
      abstract={We have discussed horizontal federated learning (HFL) and vertical federated learning (VFL) in Chapters 4 and 5, respectively. HFL requires all participating parties share the same feature space while VFL require parties share the same sample space. In practice, however, we often face situations in which there are not enough shared features or samples among the participating parties. In those cases, one can still build a federated learning model combined with transfer learning that transfers knowledge among the parties to achieve better performance. We refer to the combination of federated learning and transfer learning as Federated Transfer Learning (FTL). In this chapter, we provide a formal definition of FTL and discuss the differences between FTL and traditional transfer learning. We then introduce a secure FTL framework proposed in Liu et al. [2019], and conclude this chapter with a summary of the challenges and open issues.},
      isbn={978-3-031-01585-4},
      doi={10.1007/978-3-031-01585-4_6},
}

@inproceedings{bonawitz2017practical,
author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
title = {Practical Secure Aggregation for Privacy-Preserving Machine Learning},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3133982},
doi = {10.1145/3133956.3133982},
abstract = {We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1175-1191},
numpages = {17},
keywords = {federated learning, machine learning, privacy-preserving protocols, secure aggregation},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@misc{mcmahan2018learning,
      title={Learning Differentially Private Recurrent Language Models}, 
      author={H. Brendan McMahan and Daniel Ramage and Kunal Talwar and Li Zhang},
      year={2018},
      eprint={1710.06963},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1710.06963}, 
}

@misc{hardy2017private,
      title={Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption}, 
      author={Stephen Hardy and Wilko Henecka and Hamish Ivey-Law and Richard Nock and Giorgio Patrini and Guillaume Smith and Brian Thorne},
      year={2017},
      eprint={1711.10677},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.10677}, 
}
@misc{cheng2021secureboost,
      title={SecureBoost: A Lossless Federated Learning Framework}, 
      author={Kewei Cheng and Tao Fan and Yilun Jin and Yang Liu and Tianjian Chen and Dimitrios Papadopoulos and Qiang Yang},
      year={2021},
      eprint={1901.08755},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.08755}, 
}

@inproceedings{nikolaenko2013privacy,
author = {Nikolaenko, Valeria and Weinsberg, Udi and Ioannidis, Stratis and Joye, Marc and Boneh, Dan and Taft, Nina},
year = {2013},
month = {05},
pages = {334-348},
booktitle= {Security and Privacy},
title = {Privacy-Preserving Ridge Regression on Hundreds of Millions of Records},
isbn = {978-1-4673-6166-8},
journal = {IEEE Symposium on Security and Privacy},
doi = {10.1109/SP.2013.30}
}

@misc{bonawitz2019towards,
      title={Towards Federated Learning at Scale: System Design}, 
      author={Keith Bonawitz and Hubert Eichner and Wolfgang Grieskamp and Dzmitry Huba and Alex Ingerman and Vladimir Ivanov and Chloe Kiddon and Jakub Konečný and Stefano Mazzocchi and H. Brendan McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},
      year={2019},
      eprint={1902.01046},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.01046}, 
}

@misc{fan2022fault,
      title={Fault-Tolerant Federated Reinforcement Learning with Theoretical Guarantee}, 
      author={Flint Xiaofeng Fan and Yining Ma and Zhongxiang Dai and Wei Jing and Cheston Tan and Bryan Kian Hsiang Low},
      year={2022},
      eprint={2110.14074},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14074}, 
}

@misc{liang2020think,
      title={Think Locally, Act Globally: Federated Learning with Local and Global Representations}, 
      author={Paul Pu Liang and Terrance Liu and Liu Ziyin and Nicholas B. Allen and Randy P. Auerbach and David Brent and Ruslan Salakhutdinov and Louis-Philippe Morency},
      year={2020},
      eprint={2001.01523},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.01523}, 
}