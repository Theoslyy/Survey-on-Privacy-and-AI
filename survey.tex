% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
%
\title{Survey em IA e Segurança}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Davi Iury \and
Esther Martins \and
Lucas Pinheiro \and 
Rafael Porto \and 
Théo Araújo}
%
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Universidade Federal do Ceará}
%
\titlerunning{Survey em IA e Segurança}
\authorrunning{D. Iury et al.}
\begin{document}
\maketitle
%
\begin{abstract}
Nós resume as coisas aqui.
\keywords{IA  \and Segurança \and  Machine Learning.}
\end{abstract}
\section{Introdução}
IA é muito popular. 
Porém, precisamos de muitos dados para treinar modelos.
Como podemos arranjar esses dados?
Mais especificamente,
como podemos arrumar esses dados de forma que não infrijamos leis de privacidade de dados?
Como privacidade de dados vem se tornando um conceito cada vez mais em voga,
Novas formas de treinar modelos e obter dados vem surgindo.
Nesta survey, falaremos de: \newline
Federated learning (analisar os dados locamente e mandar os resultados de volta 
de forma criptografada) \newline
Differential privacy (é uma técnica que visa proteger a privacidade dos usuários
por meio da adição de ruído nos dados sendo analisados) \newline
Machine Unlearning (esquecer dados de usuários que foram usados para treinar modelos de 
forma que isso não prejudique o aprendizado do algoritmo).
\subsection{Contextualização}
Deixei aqui para ser o template inicial. 
Quando forem escrever,
É legal dar enter a cada oração
Para que fique dividido direito e fique fácil de ler.
\section{Caracterização Ferramental}
\subsection{Machine unlearning}
woow
\subsection{Differential privacy}
woow
\subsection{Federated learning}
\textbf{Contexto.}
É comum que algoritmos clássicos de aprendizado de máquina
mantenham centralizados os dados a serem
usados para treinamento. Isso se deve ao fato de que, frequentemente, os dados 
encontram-se dispersos em \"{}ilhas de dados\"{}\footnote{to-do referencia.}, então,
é necessário que seja feito um trabalho de captação e agrupamento em um servidor para que o uso em treinamento seja possível. 
No entanto, caso não seja feita de forma adequada, 
essa centralização facilita o vazamento de dados sensíveis.
Em vista dessa situação, 
diversas regulações vem sendo impostas com relação à captação e ao uso de dados para treinamento de modelos,
tornando o uso de técnicas de aprendizado de máquina centralizado de difícil implementação prática. 
Nesse contexto, a aplicação do federated learning possibilita com que o treinamento seja feito de forma local, 
não-centralizada, de forma que os dados de um usuário específico mantém-se somente no seu dispositivo local.  
\newline\textbf{Definindo.}
Em sua essência, o federated learning é uma técnica de aprendizado distribuído, ou seja,
ao invés de consolidarmos dados de usuário em um servidor central para treinar um modelo, haverão focos
locais de treinamento. 
Assim, evitando a captação de dados sensíveis, 
o servidor envia um modelo de treinamento para cada dispositivo individual,
mantendo a fase de treinamento como uma etapa local. Cada dispositivo somente retornará ao servidor 
central seu modelo treinado localmente e atualizará o seu modelo interno conforme as atualizações no modelo global.
Federated learning, dessa forma, garante que dados locais não possam ser vazados e, a fim de não violar 
leis gerais de proteção de dados, a troca de parâmetros entre clientes locais e o servidor 
para a geração de um modelo global é feita através de mecanismos criptografados.\footnote{botar uma figura aqui}
% In the practical application scenario [8], it is assumed that
% N users \{{}U1, $\cdots$, Un\}{} own their own database {D1, $\cdots$, Dn}, and
% each of them cannot directly access to other people's data to
% expand their own data. As shown in Fig. 1, federated learning is to
% learn a model by collecting training information from distributed
% devices. It contains three basic steps [10]: (1) Server sends the
% initial model to each device. (2) The device Ui does not need to
% share its own source data, but can federally train its own model
% Wi with the local data Di. (3) Server aggregates the collected local
% models \{{}W1, $\cdots$, Wn\}{} to the global model W\'{}, and then update 
% global model to replace each user's local model. With the rapid
% development of federated learning, the efficiency and accuracy
% of federated training models are getting closer and closer to
% centralized training models [11]. It is playing an important role
% in many areas that need to take into account privacy. 
\newline\textbf{Tipos.}
\newline\textit{1. Com relação à partição dos dados.}\newline
a. Sistemas que fornecem os dados tem usuários muitos diferentes, mas features parecidas
b. Sistemas tem features muito diversas, mas usuários muito parecidos
c. Sistemas fornecem dados muito incompatíveis (features e usuários diferentes) ou insuficientes. 
\newline\textit{2. Com relação à mecanismos de privacidade}\newline
\newline\textit{3. Com relação ao modelo de ML aplicado}\newline
\newline\textit{4. Com relação ao método de solucionar heterogênidade.}\newline
\newline\textbf{Aplicações práticas.}
\section{Geração de Dados Sintéticos e Segurança}

A escassez de dados de alta qualidade, combinada com regulamentações rigorosas de privacidade e o alto custo de anotação manual, impulsionou a adoção da geração de dados sintéticos como uma alternativa viável aos \textit{datasets} reais \cite{lu2023}. Dados sintéticos são definidos como informações artificialmente geradas por algoritmos ou simulações que mimetizam as propriedades estatísticas e comportamentais dos dados reais, sem conter informações diretamente identificáveis. No contexto de sistemas de Inteligência Artificial seguros e privados, a geração de dados sintéticos não atua apenas como uma ferramenta de aumento de dados (\textit{data augmentation}), mas como um mecanismo fundamental de preservação de privacidade e robustez \cite{goyal2024}.

\subsection{Técnicas de Geração Baseadas em IA Generativa}
A literatura recente categoriza as abordagens de geração de dados sintéticos principalmente em três arquiteturas de aprendizado profundo:

\begin{itemize}
    \item \textbf{Redes Adversárias Generativas (GANs):} Consistem em dois modelos neurais, um gerador e um discriminador, que competem entre si. O gerador cria dados falsos e o discriminador tenta distinguir entre dados reais e falsos. GANs são amplamente utilizadas para síntese de imagens e dados tabulares, com variantes aplicadas especificamente para preservar a estrutura estatística de dados sensíveis \cite{goyal2024}.
    \item \textbf{Autoencoders Variacionais (VAEs):} Os VAEs aprendem a comprimir os dados de entrada em um espaço latente probabilístico e, em seguida, reconstróem os dados a partir desse espaço, permitindo a amostragem de novos pontos de dados que seguem a distribuição original \cite{lu2023}.
    \item \textbf{Grandes Modelos de Linguagem (LLMs):} Avanços recentes, como apontado por Nadăş et al. \cite{nadas2025}, demonstram que LLMs podem atuar como geradores de dados universais para texto e código. Através de técnicas de \textit{prompting} e refinamento iterativo, LLMs podem gerar dados rotulados e código sintético verificável via execução, auxiliando no treinamento de modelos de segurança de software.
\end{itemize}

\subsection{Sinergia com Mecanismos de Privacidade e Segurança}
A geração de dados sintéticos desempenha um papel estratégico quando integrada às técnicas de \textit{Differential Privacy}, \textit{Federated Learning} e \textit{Machine Unlearning}.

\subsubsection{Privacidade Diferencial (DP).}
A aplicação direta de modelos gerativos em dados sensíveis pode resultar em riscos de privacidade, como ataques de inferência de pertinência (\textit{membership inference attacks}). Para mitigar isso, a geração de dados sintéticos é frequentemente acoplada à Privacidade Diferencial \cite{lu2023}. O objetivo é gerar um \textit{dataset} sintético que preserve as propriedades estatísticas globais, mas garanta que a saída do modelo não dependa excessivamente de nenhum registro individual. Técnicas como DP-GAN integram ruído calibrado garantindo garantias formais de privacidade ($\epsilon$-differential privacy), permitindo que dados sintéticos sejam compartilhados publicamente com risco minimizado.

\subsubsection{Aprendizado Federado (FL).}
No Aprendizado Federado, a heterogeneidade dos dados (dados \textit{non-IID}) é um desafio crítico. A geração de dados sintéticos oferece soluções em duas frentes \cite{goyal2024}:
\begin{enumerate}
    \item \textbf{Aumento de Dados Local:} Clientes com poucos dados podem usar modelos generativos para sintetizar amostras locais, melhorando a robustez do treinamento.
    \item \textbf{Privacidade Generativa Federada:} O treinamento de GANs em ambiente federado, onde apenas os parâmetros dos geradores são compartilhados, permite que o modelo global aprenda a distribuição de dados sem acesso direto aos registros brutos.
\end{enumerate}

\subsubsection{Machine Unlearning.}
O \textit{Machine Unlearning} visa remover a influência de dados específicos de um modelo treinado. A geração de dados sintéticos atua como facilitador neste processo:
\begin{itemize}
    \item \textbf{Prevenção de Memorização:} O uso de dados sintéticos treinados com DP desde o início reduz a necessidade de \textit{unlearning} frequente, pois os dados não correspondem a indivíduos reais.
    \item \textbf{Substituição de Dados:} Em cenários onde dados reais devem ser excluídos, dados sintéticos podem ser gerados para preencher a lacuna estatística deixada pela remoção, mantendo a utilidade do modelo sem violar a privacidade \cite{nadas2025}.
\end{itemize}

\section{Desafios}

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
% \begin{thebibliography}{8}
% \end{thebibliography}
\end{document}
